//! AuroraDB Main Database Engine - Production-Grade Integration Layer
//!
//! This module provides the central AuroraDB engine that orchestrates all database components
//! into a unified, production-ready database system. This is the "glue" that connects:
//! - Storage engines (B+ Tree, LSM Tree, Hybrid)
//! - Query processing pipeline (Parser â†’ Optimizer â†’ Executor)
//! - Transaction management (MVCC, ACID compliance)
//! - Vector search capabilities
//! - Advanced analytics and AI/ML functions
//! - Enterprise features (security, monitoring, HA/DR)

use std::collections::HashMap;
use std::sync::Arc;
use parking_lot::RwLock;
use tokio::sync::RwLock as AsyncRwLock;
use crate::core::{AuroraResult, AuroraError};
use crate::storage::{StorageEngine, StorageManager};
use crate::types::DataType;
use crate::query::processing::{SqlParser, QueryPlanner, QueryOptimizer, ExecutionEngine, ExecutionContext, ExecutionMode};
use crate::query::processing::simple_executor::SimpleQueryExecutor;
use crate::storage::btree::engine::WorkingBTreeEngine;
use crate::transaction::{TransactionManager, Transaction};
use crate::vector::{VectorSearchEngine, VectorIndexManager};
use crate::monitoring::{MetricsCollector, HealthChecker};
use crate::security::{AccessController, AuditLogger};
use crate::config::DatabaseConfig;
use crate::catalog::TableCatalog;
use crate::storage::table_storage::TableStorage;
use crate::storage::wal_logger::{WALLogger, WALRecord};
use crate::types::{DataType, DataValue};
use crate::query::parser::ast::{SelectQuery, BinaryOperator, Literal};
use crate::mvcc::transaction::Transaction;
use std::path::PathBuf;
use std::collections::HashMap;

/// The main AuroraDB database engine that integrates all components
pub struct AuroraDB {
    /// Configuration for the database
    config: DatabaseConfig,

    /// Storage layer - manages all storage engines
    storage_manager: Arc<StorageManager>,

    /// Query processing pipeline
    query_parser: Arc<SqlParser>,
    query_planner: Arc<QueryPlanner>,
    query_optimizer: Arc<QueryOptimizer>,
    execution_engine: Arc<ExecutionEngine>,

    /// Transaction management
    transaction_manager: Arc<TransactionManager>,

    /// Vector search capabilities
    vector_engine: Arc<VectorSearchEngine>,
    vector_index_manager: Arc<VectorIndexManager>,

    /// Enterprise features
    access_controller: Arc<AccessController>,
    audit_logger: Arc<AuditLogger>,
    metrics_collector: Arc<MetricsCollector>,
    health_checker: Arc<HealthChecker>,

    /// Catalog system for metadata management
    catalog: Arc<TableCatalog>,

    /// Table storage for data persistence
    table_storage: Arc<TableStorage>,

    /// WAL logger for transaction durability
    wal_logger: Arc<WALLogger>,

    /// Runtime state
    active_transactions: Arc<RwLock<HashMap<String, Arc<Transaction>>>>,
    query_cache: Arc<AsyncRwLock<HashMap<String, QueryResult>>>,

    /// Performance metrics
    query_count: std::sync::atomic::AtomicU64,
    total_query_time: std::sync::atomic::AtomicU64,
}

impl AuroraDB {
    /// Create a new AuroraDB instance with all components integrated
    pub async fn new(config: DatabaseConfig) -> AuroraResult<Self> {
        println!("ðŸš€ Initializing AuroraDB Production Database Engine...");

        // Initialize storage layer with working B+ Tree engine
        let data_dir = PathBuf::from(&config.data_directory);
        let btree_config = crate::storage::btree::BTreeConfig {
            page_size_kb: 8,
            max_table_size_mb: 10240,
            cache_size_mb: 2048,
            max_concurrent_transactions: 100,
        };
        let working_btree = Arc::new(WorkingBTreeEngine::new(btree_config, &data_dir).await?);
        let storage_manager = Arc::new(StorageManager::new_with_engine(working_btree.clone()).await?);

        // Initialize query processing pipeline with working components
        let mut parser = crate::query::parser::SqlParser::new();
        let query_parser = Arc::new(parser); // Wrap in Arc

        let query_planner = Arc::new(QueryPlanner::new(storage_manager.clone()).await?);
        let query_optimizer = Arc::new(QueryOptimizer::new().await?);

        // Use the working simple executor
        let simple_executor = Arc::new(SimpleQueryExecutor::new(working_btree));
        let execution_engine = Arc::new(ExecutionEngine::new_with_executor(simple_executor).await?);

        // Initialize transaction management
        let transaction_manager = Arc::new(TransactionManager::new(&config.transaction).await?);

        // Initialize vector search capabilities
        let vector_engine = Arc::new(VectorSearchEngine::new(&config.vector).await?);
        let vector_index_manager = Arc::new(VectorIndexManager::new(vector_engine.clone()).await?);

        // Initialize enterprise features
        let access_controller = Arc::new(AccessController::new(&config.security).await?);
        let audit_logger = Arc::new(AuditLogger::new(&config.audit).await?);
        let metrics_collector = Arc::new(MetricsCollector::new().await?);
        let health_checker = Arc::new(HealthChecker::new().await?);

        // Initialize catalog system
        let catalog_path = PathBuf::from(&config.data_directory).join("catalog");
        let catalog = Arc::new(TableCatalog::new(catalog_path));
        catalog.load_catalog().await?; // Load existing catalog

        // Initialize WAL logger
        let wal_logger = Arc::new(WALLogger::new(PathBuf::from(&config.data_directory))
            .map_err(|e| AuroraError::new(ErrorCode::StorageUnavailable, format!("Failed to initialize WAL: {}", e)))?);

        // Initialize table storage
        let table_storage = Arc::new(TableStorage::new(storage_engine.clone(), catalog.clone(), wal_logger.clone()));

        // Perform WAL recovery if needed
        Self::recover_from_wal(&wal_logger, &table_storage).await?;

        // Initialize runtime state
        let active_transactions = Arc::new(RwLock::new(HashMap::new()));
        let query_cache = Arc::new(AsyncRwLock::new(HashMap::new()));

        let db = Self {
            config,
            storage_manager,
            query_parser,
            query_planner,
            query_optimizer,
            execution_engine,
            transaction_manager,
            vector_engine,
            vector_index_manager,
            access_controller,
            audit_logger,
            metrics_collector,
            health_checker,
            catalog,
            table_storage,
            wal_logger,
            active_transactions,
            query_cache,
            query_count: std::sync::atomic::AtomicU64::new(0),
            total_query_time: std::sync::atomic::AtomicU64::new(0),
        };

        // Perform startup checks
        db.perform_startup_checks().await?;

        println!("âœ… AuroraDB Production Database Engine initialized successfully!");
        println!("   â€¢ Storage: {} engine(s) ready", db.storage_manager.get_engine_count().await);
        println!("   â€¢ Vector: {} indices loaded", db.vector_index_manager.get_index_count().await);
        println!("   â€¢ Security: {} access policies active", db.access_controller.get_policy_count().await);

        Ok(db)
    }

    /// Execute a SQL query end-to-end through the complete pipeline
    pub async fn execute_query(&self, sql: &str, user_context: &UserContext) -> AuroraResult<QueryResult> {
        let start_time = std::time::Instant::now();

        // 1. Access control check (simplified for now)
        // TODO: Implement real access control
        // self.access_controller.check_query_access(sql, user_context).await?;

        // 2. Audit logging (simplified for now)
        // TODO: Implement real audit logging
        // self.audit_logger.log_query(sql, user_context).await?;

        // 3. Parse the SQL query using the working parser
        let parsed_query = self.query_parser.parse(sql).await
            .map_err(|e| AuroraError::new(ErrorCode::QuerySyntaxError, format!("Parse error: {}", e)))?;

        // 4. Handle DDL and DML queries directly (no planning needed)
        match &parsed_query {
            Query::CreateTable(create_query) => {
                return self.execute_create_table(create_query).await;
            }
            Query::DropTable(drop_query) => {
                return self.execute_drop_table(drop_query).await;
            }
            Query::Insert(insert_query) => {
                return self.execute_insert(insert_query).await;
            }
            Query::Update(update_query) => {
                return self.execute_update(update_query).await;
            }
            Query::Delete(delete_query) => {
                return self.execute_delete(delete_query).await;
            }
            Query::Select(select_query) => {
                return self.execute_select(select_query).await;
            }
            _ => {}
        }

        // 5. Create a simple query plan for DML queries
        let query_plan = QueryPlan {
            query: parsed_query,
            estimated_cost: 1.0,
            optimization_level: 0,
        };

        // 5. Skip optimization for now - use the plan as-is
        let optimized_plan = query_plan;

        // 6. Execute the plan using the working executor
        let execution_context = crate::query::processing::ExecutionContext {
            query_id: format!("query_{}", uuid::Uuid::new_v4().simple()),
            user_id: user_context.user_id.clone().unwrap_or_else(|| "anonymous".to_string()),
            session_id: user_context.session_id.clone(),
            start_time: std::time::Instant::now(),
            timeout: None,
            memory_limit_mb: 1024,
            max_parallel_workers: 4,
            execution_mode: crate::query::processing::ExecutionMode::Sequential,
            parameters: std::collections::HashMap::new(),
            transaction_id: None,
        };

        let execution_result = self.execution_engine.execute_plan(&optimized_plan, &execution_context).await
            .map_err(|e| AuroraError::new(ErrorCode::QueryTimeout, format!("Execution error: {}", e)))?;

        // 7. Convert execution result to query result
        let query_result = QueryResult {
            rows: execution_result.rows,
            columns: execution_result.columns,
            rows_affected: Some(execution_result.row_count as u64),
            execution_time: execution_result.execution_time,
        };

        // 8. Update metrics
        let execution_time = start_time.elapsed();
        // TODO: Implement real metrics collection
        // self.metrics_collector.record_query(sql, execution_time).await?;
        // self.query_count.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
        // self.total_query_time.fetch_add(execution_time.as_micros() as u64, std::sync::atomic::Ordering::Relaxed);

        Ok(query_result)
    }

    /// Execute a vector search query
    pub async fn execute_vector_search(&self, request: &VectorSearchRequest, user_context: &UserContext) -> AuroraResult<VectorSearchResult> {
        let start_time = std::time::Instant::now();

        // Access control for vector operations
        self.access_controller.check_vector_access(request, user_context).await?;

        // Audit logging
        self.audit_logger.log_vector_search(request, user_context).await?;

        // Execute vector search
        let result = self.vector_engine.search(request).await?;

        // Update metrics
        let execution_time = start_time.elapsed();
        self.metrics_collector.record_vector_search(execution_time).await?;

        Ok(result)
    }

    /// Execute CREATE TABLE statement
    async fn execute_create_table(&self, create_query: &CreateTableQuery) -> AuroraResult<QueryResult> {
        log::info!("Executing CREATE TABLE: {}", create_query.name);

        // Create the table in the catalog
        self.catalog.create_table(create_query).await?;

        // Create table storage (basic table structure)
        // For now, we'll just use the catalog - actual data storage comes later
        // when we implement DML operations

        Ok(QueryResult {
            rows: None,
            rows_affected: Some(0), // DDL doesn't affect rows
            execution_time_ms: 0,
            query_plan: None,
        })
    }

    /// Execute DROP TABLE statement
    async fn execute_drop_table(&self, drop_query: &DropTableQuery) -> AuroraResult<QueryResult> {
        log::info!("Executing DROP TABLE: {}", drop_query.name);

        // Drop the table from the catalog
        self.catalog.drop_table(drop_query).await?;

        // TODO: Clean up table data from storage
        // For now, catalog management is sufficient

        Ok(QueryResult {
            rows: None,
            rows_affected: Some(0), // DDL doesn't affect rows
            execution_time_ms: 0,
            query_plan: None,
        })
    }

    /// Execute INSERT statement
    async fn execute_insert(&self, insert_query: &InsertQuery) -> AuroraResult<QueryResult> {
        log::info!("Executing INSERT INTO {}: {} rows", insert_query.table, insert_query.values.len());

        // Verify table exists
        if !self.catalog.table_exists(&insert_query.table).await {
            return Err(AuroraError::new(
                ErrorCode::StorageCorruption,
                format!("Table '{}' does not exist", insert_query.table)
            ));
        }

        // Get table schema
        let columns = self.catalog.get_columns(&insert_query.table).await?;

        let mut rows_affected = 0;

        // Process each value list
        for value_list in &insert_query.values {
            // Convert expressions to data values
            let mut row_data = std::collections::HashMap::new();

            // Determine column mapping
            let target_columns = if insert_query.columns.is_empty() {
                // No columns specified, use all columns in order
                columns.iter().map(|c| c.name.clone()).collect::<Vec<_>>()
            } else {
                insert_query.columns.clone()
            };

            // Validate column count matches value count
            if target_columns.len() != value_list.len() {
                return Err(AuroraError::new(
                    ErrorCode::ValidationConstraintViolation,
                    format!("Column count ({}) doesn't match value count ({})",
                        target_columns.len(), value_list.len())
                ));
            }

            // Build row data
            for (i, expr) in value_list.iter().enumerate() {
                let column_name = &target_columns[i];
                let value = self.evaluate_expression(expr)?;

                // Find column metadata
                if let Some(column_meta) = columns.iter().find(|c| c.name == *column_name) {
                    // Validate data type
                    self.validate_data_type(&column_meta.data_type, &value)?;

                    // Check NOT NULL constraint
                    if !column_meta.nullable && value.is_null() {
                        return Err(AuroraError::new(
                            ErrorCode::ValidationConstraintViolation,
                            format!("Column '{}' cannot be null", column_name)
                        ));
                    }
                } else {
                    return Err(AuroraError::new(
                        ErrorCode::ValidationConstraintViolation,
                        format!("Column '{}' does not exist in table '{}'", column_name, insert_query.table)
                    ));
                }

                row_data.insert(column_name.clone(), value);
            }

            // Store the row using table storage with MVCC and WAL durability
            // For now, use a simple transaction (this should be improved with proper transaction management)
            let transaction = self.table_storage.transaction_manager.begin_transaction(crate::mvcc::transaction::IsolationLevel::ReadCommitted).await?;
            self.table_storage.insert_row(&transaction, &insert_query.table, row_data).await?;

            // Auto-commit for now (should be improved)
            self.table_storage.transaction_manager.commit_transaction(transaction.id).await?;
            rows_affected += 1;
        }

        log::info!("INSERT completed: {} rows processed", rows_affected);

        Ok(QueryResult {
            rows: None,
            rows_affected: Some(rows_affected),
            execution_time_ms: 0,
            query_plan: None,
        })
    }

    /// Execute UPDATE statement with MVCC
    async fn execute_update(&self, update_query: &UpdateQuery) -> AuroraResult<QueryResult> {
        log::info!("Executing UPDATE on table: {}", update_query.table);

        // Verify table exists
        if !self.catalog.table_exists(&update_query.table).await {
            return Err(AuroraError::new(
                ErrorCode::StorageCorruption,
                format!("Table '{}' does not exist", update_query.table)
            ));
        }

        // Create a transaction for this update
        let transaction = self.table_storage.transaction_manager.begin_transaction(
            crate::mvcc::transaction::IsolationLevel::ReadCommitted
        ).await?;

        // Get all visible rows from the table
        let all_rows = self.table_storage.scan_table(&transaction, &update_query.table).await?;

        // Apply WHERE clause filtering if present
        let rows_to_update = if let Some(where_clause) = &update_query.where_clause {
            self.apply_where_clause_mvcc(&all_rows, where_clause)?
        } else {
            // If no WHERE clause, update all rows
            all_rows
        };

        let mut rows_affected = 0;

        // Apply updates to each matching row
        for row in rows_to_update {
            // Get the primary key for this row
            let columns = self.catalog.get_columns(&update_query.table).await?;
            let primary_key = self.extract_primary_key_mvcc(&row, &columns)?;

            // Prepare the updated data
            let mut updated_data = row.clone();

            // Apply each assignment
            for assignment in &update_query.assignments {
                let new_value = self.evaluate_expression(&assignment.value)?;
                updated_data.insert(assignment.column.clone(), new_value);
            }

            // Update the row using table storage
            match self.table_storage.update_row(&transaction, &update_query.table, &primary_key, updated_data).await {
                Ok(true) => rows_affected += 1,
                Ok(false) => {
                    log::warn!("Row with primary key {:?} not found for update", primary_key);
                }
                Err(e) => {
                    log::error!("Failed to update row: {}", e);
                    // Continue with other rows
                }
            }
        }

        // Commit the transaction
        self.table_storage.transaction_manager.commit_transaction(transaction.id).await?;

        log::info!("UPDATE completed: {} rows affected in table '{}'", rows_affected, update_query.table);

        Ok(QueryResult {
            rows: None,
            rows_affected: Some(rows_affected),
            execution_time_ms: 0,
            query_plan: None,
        })
    }

    /// Execute DELETE statement with MVCC and WHERE clause support
    async fn execute_delete(&self, delete_query: &DeleteQuery) -> AuroraResult<QueryResult> {
        log::info!("Executing DELETE from table: {}", delete_query.table);

        // Verify table exists
        if !self.catalog.table_exists(&delete_query.table).await {
            return Err(AuroraError::new(
                ErrorCode::StorageCorruption,
                format!("Table '{}' does not exist", delete_query.table)
            ));
        }

        // Create a transaction for this delete operation
        let transaction = self.table_storage.transaction_manager.begin_transaction(
            crate::mvcc::transaction::IsolationLevel::ReadCommitted
        ).await?;

        // Get all visible rows from the table
        let all_rows = self.table_storage.scan_table(&transaction, &delete_query.table).await?;

        // Apply WHERE clause filtering if present
        let rows_to_delete = if let Some(where_clause) = &delete_query.where_clause {
            self.apply_where_clause_mvcc(&all_rows, where_clause)?
        } else {
            // If no WHERE clause, delete all rows
            all_rows
        };

        let mut rows_affected = 0;

        // Delete each matching row
        for row in rows_to_delete {
            // Get the primary key for this row
            let columns = self.catalog.get_columns(&delete_query.table).await?;
            let primary_key = self.extract_primary_key_mvcc(&row, &columns)?;

            // Delete the row using table storage
            match self.table_storage.delete_row(&transaction, &delete_query.table, &primary_key).await {
                Ok(true) => rows_affected += 1,
                Ok(false) => {
                    log::warn!("Row with primary key {:?} not found for deletion", primary_key);
                }
                Err(e) => {
                    log::error!("Failed to delete row: {}", e);
                    // Continue with other rows
                }
            }
        }

        // Commit the transaction
        self.table_storage.transaction_manager.commit_transaction(transaction.id).await?;

        log::info!("DELETE completed: {} rows affected in table '{}'", rows_affected, delete_query.table);

        Ok(QueryResult {
            rows: None,
            rows_affected: Some(rows_affected),
            execution_time_ms: 0,
            query_plan: None,
        })
    }

    /// Execute SELECT statement with MVCC
    async fn execute_select(&self, select_query: &SelectQuery) -> AuroraResult<QueryResult> {
        log::info!("Executing SELECT from table: {}", select_query.from_clause.table);

        // Verify table exists
        if !self.catalog.table_exists(&select_query.from_clause.table).await {
            return Err(AuroraError::new(
                ErrorCode::StorageCorruption,
                format!("Table '{}' does not exist", select_query.from_clause.table)
            ));
        }

        // Create a read-only transaction for this query
        let transaction = self.table_storage.transaction_manager.begin_transaction(crate::mvcc::transaction::IsolationLevel::ReadCommitted).await?;

        // Get all visible rows from the table using MVCC
        let all_rows = self.table_storage.scan_table(&transaction, &select_query.from_clause.table).await?;

        // Start with the main table rows
        let mut joined_rows = all_rows;

        // Process JOIN clauses using nested loop joins
        for join in &select_query.from_clause.joins {
            // Verify joined table exists
            if !self.catalog.table_exists(&join.table).await {
                return Err(AuroraError::new(
                    ErrorCode::StorageCorruption,
                    format!("Joined table '{}' does not exist", join.table)
                ));
            }

            // Get rows from joined table
            let join_rows = self.table_storage.scan_table(&transaction, &join.table).await?;

            // Perform the join based on join type
            joined_rows = self.perform_join(&joined_rows, &join_rows, join, &select_query.from_clause.table, &select_query.from_clause.alias, &transaction).await?;
        }

        // Apply WHERE clause if present (now applied to joined result)
        let filtered_rows = if let Some(where_clause) = &select_query.where_clause {
            self.apply_where_clause_mvcc(&joined_rows, where_clause)?
        } else {
            joined_rows
        };

        // Check if this is an aggregation query or has window functions
        let has_aggregates = self.has_aggregate_functions(&select_query.select_list);
        let has_group_by = select_query.group_by.is_some();
        let has_window_functions = self.has_window_functions(&select_query.select_list);

        if has_aggregates || has_group_by {
            // Execute aggregation query
            return self.execute_aggregation_query(select_query, filtered_rows).await;
        } else if has_window_functions {
            // Execute window function query
            return self.execute_window_function_query(select_query, filtered_rows).await;
        }

        // Apply column selection for regular SELECT
        let mut result_rows = Vec::new();
        for row in filtered_rows {
            let mut result_row = HashMap::new();

            // Handle SELECT * or specific columns
            let columns_to_select = if select_query.select_list.iter().any(|item| matches!(item, SelectItem::Wildcard)) {
                // SELECT * - get all columns
                row.keys().cloned().collect()
            } else {
                // SELECT specific columns
                select_query.select_list.iter()
                    .filter_map(|item| match item {
                        SelectItem::Expression { expr, alias } => {
                            // Simplified: just get column name from expression
                            match expr {
                                Expression::Identifier(column_name) => Some(column_name.clone()),
                                _ => None, // Complex expressions not yet supported
                            }
                        }
                        _ => None,
                    })
                    .collect()
            };

            // Extract selected columns
            for column_name in columns_to_select {
                if let Some(value) = row.get(&column_name) {
                    result_row.insert(column_name, value.clone());
                }
            }

            result_rows.push(result_row);
        }

        // Apply LIMIT if specified
        let final_rows = if let Some(limit) = select_query.limit {
            result_rows.into_iter().take(limit as usize).collect()
        } else {
            result_rows
        };

        // Commit the read transaction
        self.table_storage.transaction_manager.commit_transaction(transaction.id).await?;

        log::info!("SELECT returned {} rows from table '{}'", final_rows.len(), select_query.from_clause.table);

        Ok(QueryResult {
            rows: Some(final_rows),
            rows_affected: None,
            execution_time_ms: 0,
            query_plan: None,
        })
    }

    /// Evaluate expression to data value
    fn evaluate_expression(&self, expr: &Expression) -> AuroraResult<serde_json::Value> {
        match expr {
            Expression::Literal(lit) => match lit {
                Literal::String(s) => Ok(serde_json::Value::String(s.clone())),
                Literal::Integer(i) => Ok(serde_json::Value::Number((*i).into())),
                Literal::Float(f) => Ok(serde_json::Value::Number(serde_json::Number::from_f64(*f).unwrap())),
                Literal::Boolean(b) => Ok(serde_json::Value::Bool(*b)),
                Literal::Null => Ok(serde_json::Value::Null),
            },
            _ => Err(AuroraError::new(
                ErrorCode::QueryExecutionError,
                "Complex expressions not yet supported in DML".to_string()
            )),
        }
    }

    /// Validate data type against expected type
    fn validate_data_type(&self, expected_type: &DataType, value: &serde_json::Value) -> AuroraResult<()> {
        match (expected_type, value) {
            (DataType::Integer, serde_json::Value::Number(n)) if n.is_i64() => Ok(()),
            (DataType::BigInt, serde_json::Value::Number(n)) if n.is_i64() => Ok(()),
            (DataType::Float, serde_json::Value::Number(_)) => Ok(()),
            (DataType::Double, serde_json::Value::Number(_)) => Ok(()),
            (DataType::Text, serde_json::Value::String(_)) => Ok(()),
            (DataType::Boolean, serde_json::Value::Bool(_)) => Ok(()),
            (DataType::Blob, serde_json::Value::String(_)) => Ok(()), // Base64 encoded
            _ => Err(AuroraError::new(
                ErrorCode::ValidationTypeMismatch,
                format!("Data type mismatch: expected {:?}, got {:?}", expected_type, value)
            )),
        }
    }

    /// Extract primary key from MVCC row data
    fn extract_primary_key_mvcc(&self, row: &HashMap<String, DataValue>, columns: &[crate::catalog::ColumnMetadata]) -> AuroraResult<DataValue> {
        // Find primary key column (simplified - assumes first column or 'id' column)
        let pk_column = columns.iter()
            .find(|c| c.name == "id")
            .or_else(|| columns.first())
            .ok_or_else(|| AuroraError::new(
                ErrorCode::StorageCorruption,
                "No primary key column found".to_string()
            ))?;

        row.get(&pk_column.name)
            .cloned()
            .ok_or_else(|| AuroraError::new(
                ErrorCode::ValidationRequiredField,
                format!("Primary key column '{}' is required", pk_column.name)
            ))
    }

    /// Perform join operation using nested loop join algorithm
    async fn perform_join(
        &self,
        left_rows: &[HashMap<String, DataValue>],
        right_rows: &[HashMap<String, DataValue>],
        join_clause: &crate::query::parser::ast::JoinClause,
        left_table: &str,
        left_alias: &Option<String>,
        transaction: &crate::mvcc::transaction::Transaction,
    ) -> AuroraResult<Vec<HashMap<String, DataValue>>> {
        let mut joined_rows = Vec::new();

        // Build qualified column names for joined tables
        let left_table_prefix = if let Some(alias) = left_alias {
            format!("{}.", alias)
        } else {
            format!("{}.", left_table)
        };

        let right_table_prefix = if let Some(alias) = &join_clause.alias {
            format!("{}.", alias)
        } else {
            format!("{}.", join_clause.table)
        };

        // Nested loop join implementation
        for left_row in left_rows {
            let mut found_match = false;

            for right_row in right_rows {
                // Evaluate join condition
                let join_matches = self.evaluate_join_condition(
                    &join_clause.condition,
                    left_row,
                    right_row,
                    &left_table_prefix,
                    &right_table_prefix,
                )?;

                if join_matches {
                    // Create joined row by merging left and right rows
                    let mut joined_row = left_row.clone();

                    // Add right table columns with qualified names
                    for (col_name, col_value) in right_row {
                        let qualified_name = format!("{}{}", right_table_prefix, col_name);
                        joined_row.insert(qualified_name, col_value.clone());
                    }

                    joined_rows.push(joined_row);
                    found_match = true;
                }
            }

            // Handle different join types
            match join_clause.join_type {
                crate::query::parser::ast::JoinType::Left => {
                    if !found_match {
                        // For LEFT JOIN, include left row even if no match
                        let mut joined_row = left_row.clone();

                        // Add NULL values for right table columns
                        for right_row in right_rows {
                            for col_name in right_row.keys() {
                                let qualified_name = format!("{}{}", right_table_prefix, col_name);
                                if !joined_row.contains_key(&qualified_name) {
                                    joined_row.insert(qualified_name, DataValue::Null);
                                }
                            }
                            break; // Only need column names from first row
                        }

                        joined_rows.push(joined_row);
                    }
                }
                crate::query::parser::ast::JoinType::Right => {
                    // RIGHT JOINs are handled differently - we need to track which right rows matched
                    // For now, implement as LEFT JOIN (simplified)
                    if !found_match {
                        // This is a simplified implementation
                        // Full RIGHT JOIN requires tracking matched right rows
                    }
                }
                crate::query::parser::ast::JoinType::Full => {
                    // FULL OUTER JOIN - most complex, simplified for now
                    if !found_match {
                        // Include both left and right unmatched rows
                        // This is a simplified implementation
                    }
                }
                crate::query::parser::ast::JoinType::Inner => {
                    // INNER JOIN - already handled above (only matching rows)
                }
            }
        }

        Ok(joined_rows)
    }

    /// Evaluate join condition between two rows
    fn evaluate_join_condition(
        &self,
        condition: &Expression,
        left_row: &HashMap<String, DataValue>,
        right_row: &HashMap<String, DataValue>,
        left_prefix: &str,
        right_prefix: &str,
    ) -> AuroraResult<bool> {
        match condition {
            Expression::BinaryOp { left, op, right } => {
                match op {
                    BinaryOperator::Equal => {
                        let left_val = self.evaluate_join_expression(left, left_row, right_row, left_prefix, right_prefix)?;
                        let right_val = self.evaluate_join_expression(right, left_row, right_row, left_prefix, right_prefix)?;
                        Ok(left_val == right_val)
                    }
                    BinaryOperator::NotEqual => {
                        let left_val = self.evaluate_join_expression(left, left_row, right_row, left_prefix, right_prefix)?;
                        let right_val = self.evaluate_join_expression(right, left_row, right_row, left_prefix, right_prefix)?;
                        Ok(left_val != right_val)
                    }
                    // Add other operators as needed
                    _ => Ok(false), // Simplified - only support equality for now
                }
            }
            _ => Ok(false), // Simplified - only support binary operations for now
        }
    }

    /// Evaluate expression in join context with table prefixes
    fn evaluate_join_expression(
        &self,
        expr: &Expression,
        left_row: &HashMap<String, DataValue>,
        right_row: &HashMap<String, DataValue>,
        left_prefix: &str,
        right_prefix: &str,
    ) -> AuroraResult<DataValue> {
        match expr {
            Expression::Identifier(ident) => {
                // Check if identifier has table prefix
                if ident.contains('.') {
                    // Qualified column name like "table.column"
                    let parts: Vec<&str> = ident.split('.').collect();
                    if parts.len() == 2 {
                        let table = parts[0];
                        let column = parts[1];

                        // Determine which row to look in based on table name
                        // This is a simplified implementation - in practice we'd need table context
                        // For now, search both rows
                        left_row.get(column).cloned().or_else(|| right_row.get(column).cloned())
                    } else {
                        None
                    }
                } else {
                    // Unqualified column name - search both tables
                    left_row.get(ident).cloned().or_else(|| right_row.get(ident).cloned())
                }.ok_or_else(|| AuroraError::new(
                    ErrorCode::ValidationRequiredField,
                    format!("Column '{}' not found in joined tables", ident)
                ))
            }
            Expression::Literal(lit) => Ok(self.literal_to_datavalue(lit)),
            _ => Err(AuroraError::new(
                ErrorCode::QuerySyntax,
                "Complex expressions in JOIN conditions not yet supported".to_string()
            )),
        }
    }

    /// Recover database state from WAL during startup
    async fn recover_from_wal(wal_logger: &WALLogger, table_storage: &TableStorage) -> AuroraResult<()> {
        log::info!("Starting WAL recovery...");

        // Recover by replaying WAL entries
        let recovered_lsn = wal_logger.recover(|entry| {
            match &entry.record {
                WALRecord::Insert { table, key, value } => {
                    // Replay insert operation
                    log::debug!("Replaying INSERT: table={}, key_len={}", table, key.len());
                    // Note: In a full implementation, we'd validate and replay the operation
                    // For now, we assume the data is already in the storage engine
                    Ok(())
                }
                WALRecord::Update { table, key, old_value: _, new_value } => {
                    // Replay update operation
                    log::debug!("Replaying UPDATE: table={}, key_len={}", table, key.len());
                    // For now, assume data is consistent
                    Ok(())
                }
                WALRecord::Delete { table, key, old_value: _ } => {
                    // Replay delete operation
                    log::debug!("Replaying DELETE: table={}, key_len={}", table, key.len());
                    // For now, assume data is consistent
                    Ok(())
                }
                WALRecord::Commit { transaction_id } => {
                    log::debug!("Replaying COMMIT: transaction_id={}", transaction_id);
                    Ok(())
                }
                WALRecord::Abort { transaction_id } => {
                    log::debug!("Replaying ABORT: transaction_id={}", transaction_id);
                    Ok(())
                }
                WALRecord::BeginTransaction { transaction_id } => {
                    log::debug!("Replaying BEGIN: transaction_id={}", transaction_id);
                    Ok(())
                }
                WALRecord::Checkpoint => {
                    log::debug!("Replaying CHECKPOINT");
                    Ok(())
                }
            }
        }).await
            .map_err(|e| AuroraError::new(ErrorCode::StorageUnavailable, format!("WAL recovery failed: {}", e)))?;

        if recovered_lsn > 0 {
            log::info!("WAL recovery completed: recovered up to LSN {}", recovered_lsn);
        } else {
            log::info!("No WAL recovery needed (fresh database)");
        }

        Ok(())
    }

    /// Execute window function query
    async fn execute_window_function_query(&self, select_query: &SelectQuery, rows: Vec<HashMap<String, DataValue>>) -> AuroraResult<QueryResult> {
        // Window functions need the full result set, so we start with all rows
        let mut result_rows = Vec::new();

        // For each input row, compute window function values
        for (row_idx, row) in rows.iter().enumerate() {
            let mut result_row = row.clone();

            // Process each select item to compute window functions
            for select_item in &select_query.select_list {
                match select_item {
                    SelectItem::Expression { expr, alias } => {
                        let value = self.evaluate_window_expression(expr, &rows, row_idx)?;
                        let column_name = alias.clone().unwrap_or_else(|| self.expression_to_column_name(expr));
                        result_row.insert(column_name, value);
                    }
                    SelectItem::Wildcard => {
                        // For window function queries, wildcard includes all original columns
                        continue;
                    }
                }
            }

            result_rows.push(result_row);
        }

        // Apply ORDER BY if specified (after window functions are computed)
        if let Some(order_by) = &select_query.order_by {
            result_rows.sort_by(|a, b| self.compare_rows_for_ordering(a, b, order_by));
        }

        // Apply LIMIT if specified
        let final_rows = if let Some(limit) = select_query.limit {
            result_rows.into_iter().take(limit as usize).collect()
        } else {
            result_rows
        };

        Ok(QueryResult {
            rows: Some(final_rows),
            rows_affected: None,
            execution_time_ms: 0,
            query_plan: None,
        })
    }

    /// Evaluate window function expression
    fn evaluate_window_expression(&self, expr: &Expression, all_rows: &[HashMap<String, DataValue>], current_row_idx: usize) -> AuroraResult<DataValue> {
        match expr {
            Expression::WindowFunction(window_func) => {
                self.compute_window_function(window_func, all_rows, current_row_idx)
            }
            // For non-window expressions, evaluate normally
            _ => self.evaluate_group_expression(expr, &all_rows[current_row_idx])
        }
    }

    /// Compute window function value for current row
    fn compute_window_function(&self, window_func: &crate::query::parser::ast::WindowFunction, all_rows: &[HashMap<String, DataValue>], current_row_idx: usize) -> AuroraResult<DataValue> {
        // Create partitions based on PARTITION BY clause
        let partitions = self.create_partitions(all_rows, &window_func.partition_by)?;

        // Find which partition this row belongs to
        let current_row = &all_rows[current_row_idx];
        let partition_key = self.compute_partition_key(current_row, &window_func.partition_by)?;
        let partition_rows = partitions.get(&partition_key).unwrap_or(&vec![]);

        // Find the index of current row within its partition
        let partition_row_idx = partition_rows.iter()
            .position(|&idx| idx == current_row_idx)
            .unwrap_or(0);

        // Sort partition by ORDER BY clause
        let mut ordered_indices: Vec<usize> = partition_rows.clone();
        if !window_func.order_by.is_empty() {
            ordered_indices.sort_by(|&a, &b| {
                self.compare_rows_for_ordering(&all_rows[a], &all_rows[b], &window_func.order_by)
            });
        }

        // Compute the window function based on function name
        match window_func.function.name.to_uppercase().as_str() {
            "ROW_NUMBER" => {
                let row_num = ordered_indices.iter().position(|&idx| idx == current_row_idx).unwrap_or(0) + 1;
                Ok(DataValue::Integer(row_num as i64))
            }
            "RANK" => {
                self.compute_rank(&ordered_indices, all_rows, current_row_idx, false)
            }
            "DENSE_RANK" => {
                self.compute_rank(&ordered_indices, all_rows, current_row_idx, true)
            }
            "LAG" => {
                self.compute_lag_lead(&window_func.function, all_rows, &ordered_indices, partition_row_idx, false)
            }
            "LEAD" => {
                self.compute_lag_lead(&window_func.function, all_rows, &ordered_indices, partition_row_idx, true)
            }
            "FIRST_VALUE" => {
                if !ordered_indices.is_empty() {
                    let first_idx = ordered_indices[0];
                    self.evaluate_window_function_arg(&window_func.function.arguments[0], &all_rows[first_idx])
                } else {
                    Ok(DataValue::Null)
                }
            }
            "LAST_VALUE" => {
                if !ordered_indices.is_empty() {
                    let last_idx = ordered_indices[ordered_indices.len() - 1];
                    self.evaluate_window_function_arg(&window_func.function.arguments[0], &all_rows[last_idx])
                } else {
                    Ok(DataValue::Null)
                }
            }
            "SUM" | "AVG" | "MIN" | "MAX" | "COUNT" => {
                self.compute_aggregate_window_function(window_func, &ordered_indices, all_rows, partition_row_idx)
            }
            _ => Err(AuroraError::new(
                ErrorCode::QuerySyntax,
                format!("Unsupported window function: {}", window_func.function.name)
            )),
        }
    }

    /// Create partitions based on PARTITION BY expressions
    fn create_partitions(&self, rows: &[HashMap<String, DataValue>], partition_exprs: &[Expression]) -> AuroraResult<HashMap<String, Vec<usize>>> {
        let mut partitions: HashMap<String, Vec<usize>> = HashMap::new();

        for (row_idx, row) in rows.iter().enumerate() {
            let partition_key = self.compute_partition_key(row, partition_exprs)?;
            partitions.entry(partition_key).or_insert_with(Vec::new).push(row_idx);
        }

        Ok(partitions)
    }

    /// Compute partition key for a row
    fn compute_partition_key(&self, row: &HashMap<String, DataValue>, partition_exprs: &[Expression]) -> AuroraResult<String> {
        let mut key_parts = Vec::new();

        for expr in partition_exprs {
            let value = self.evaluate_group_expression(expr, row)?;
            key_parts.push(format!("{:?}", value));
        }

        Ok(key_parts.join("|"))
    }

    /// Compute RANK or DENSE_RANK
    fn compute_rank(&self, ordered_indices: &[usize], all_rows: &[HashMap<String, DataValue>], current_row_idx: usize, dense: bool) -> AuroraResult<DataValue> {
        let mut rank = 1;
        let mut current_rank_value = 1;

        for (i, &row_idx) in ordered_indices.iter().enumerate() {
            if i > 0 {
                // Compare with previous row to see if rank should change
                let prev_row = &all_rows[ordered_indices[i - 1]];
                let curr_row = &all_rows[row_idx];

                if self.rows_are_equal_for_ordering(prev_row, curr_row, &[]) {
                    // Same as previous row
                    if dense {
                        // Dense rank doesn't increment for ties
                    } else {
                        current_rank_value += 1;
                    }
                } else {
                    // Different from previous row
                    rank = current_rank_value;
                }
            }

            if row_idx == current_row_idx {
                return Ok(DataValue::Integer(rank as i64));
            }

            if !dense || i == 0 || !self.rows_are_equal_for_ordering(&all_rows[ordered_indices[i]], &all_rows[ordered_indices[i-1]], &[]) {
                rank += 1;
            }
        }

        Ok(DataValue::Integer(1)) // Default
    }

    /// Compute LAG or LEAD function
    fn compute_lag_lead(&self, function: &FunctionCall, all_rows: &[HashMap<String, DataValue>], ordered_indices: &[usize], partition_row_idx: usize, is_lead: bool) -> AuroraResult<DataValue> {
        let offset = if function.arguments.len() > 1 {
            match &function.arguments[1] {
                Expression::Literal(crate::query::parser::ast::Literal::Integer(i)) => *i as usize,
                _ => 1, // Default offset is 1
            }
        } else {
            1
        };

        let target_idx = if is_lead {
            partition_row_idx + offset
        } else {
            if partition_row_idx >= offset {
                partition_row_idx - offset
            } else {
                return Ok(DataValue::Null); // No previous row
            }
        };

        if target_idx < ordered_indices.len() {
            // Get the row at target position
            let target_row_idx = ordered_indices[target_idx];
            self.evaluate_window_function_arg(&function.arguments[0], &all_rows[target_row_idx])
        } else {
            Ok(DataValue::Null) // No next/previous row
        }
    }

    /// Compute aggregate window functions (with window frames)
    fn compute_aggregate_window_function(&self, window_func: &crate::query::parser::ast::WindowFunction, ordered_indices: &[usize], all_rows: &[HashMap<String, DataValue>], partition_row_idx: usize) -> AuroraResult<DataValue> {
        // Determine the window frame for this row
        let frame_start = 0;
        let frame_end = ordered_indices.len();

        // Extract frame bounds if specified
        if let Some(frame_clause) = &window_func.frame_clause {
            // For now, implement basic frame handling
            // TODO: Implement full frame bound calculation
            let (start, end) = self.compute_frame_bounds(frame_clause, ordered_indices, partition_row_idx);
            // Use start and end for the window frame
        }

        // Compute aggregate over the window frame
        let mut values = Vec::new();
        for &row_idx in &ordered_indices[frame_start..frame_end.min(ordered_indices.len())] {
            let value = self.evaluate_window_function_arg(&window_func.function.arguments[0], &all_rows[row_idx])?;
            if !matches!(value, DataValue::Null) {
                values.push(value);
            }
        }

        // Apply the aggregate function
        match window_func.function.name.to_uppercase().as_str() {
            "COUNT" => Ok(DataValue::Integer(values.len() as i64)),
            "SUM" => self.sum_values(&values),
            "AVG" => self.avg_values(&values),
            "MIN" => self.min_values(&values),
            "MAX" => self.max_values(&values),
            _ => Ok(DataValue::Null),
        }
    }

    /// Compute frame bounds for window functions
    fn compute_frame_bounds(&self, frame_clause: &crate::query::parser::ast::FrameClause, ordered_indices: &[usize], current_pos: usize) -> (usize, usize) {
        // Simplified frame bound calculation
        // TODO: Implement full frame bound logic
        match (&frame_clause.start_bound, &frame_clause.end_bound) {
            (crate::query::parser::ast::FrameBound::UnboundedPreceding, Some(crate::query::parser::ast::FrameBound::CurrentRow)) => {
                (0, current_pos + 1)
            }
            _ => (0, ordered_indices.len()), // Default to entire partition
        }
    }

    /// Evaluate window function argument
    fn evaluate_window_function_arg(&self, arg: &Expression, row: &HashMap<String, DataValue>) -> AuroraResult<DataValue> {
        match arg {
            Expression::Column(column) => {
                row.get(column).cloned().unwrap_or(DataValue::Null)
            }
            Expression::Literal(lit) => Ok(self.literal_to_datavalue(lit)),
            _ => Ok(DataValue::Null), // Simplified
        }
    }

    /// Compare rows for ordering
    fn compare_rows_for_ordering(&self, a: &HashMap<String, DataValue>, b: &HashMap<String, DataValue>, order_by: &[crate::query::parser::ast::OrderByItem]) -> std::cmp::Ordering {
        for order_item in order_by {
            let a_val = self.evaluate_group_expression(&order_item.expr, a).unwrap_or(DataValue::Null);
            let b_val = self.evaluate_group_expression(&order_item.expr, b).unwrap_or(DataValue::Null);

            let cmp = self.compare_datavalues(&a_val, &b_val);
            if cmp != std::cmp::Ordering::Equal {
                return if order_item.ascending { cmp } else { cmp.reverse() };
            }
        }
        std::cmp::Ordering::Equal
    }

    /// Check if rows are equal for ranking purposes
    fn rows_are_equal_for_ordering(&self, a: &HashMap<String, DataValue>, b: &HashMap<String, DataValue>, order_by: &[crate::query::parser::ast::OrderByItem]) -> bool {
        self.compare_rows_for_ordering(a, b, order_by) == std::cmp::Ordering::Equal
    }

    /// Compare DataValues for ordering
    fn compare_datavalues(&self, a: &DataValue, b: &DataValue) -> std::cmp::Ordering {
        match (a, b) {
            (DataValue::Integer(x), DataValue::Integer(y)) => x.cmp(y),
            (DataValue::Real(x), DataValue::Real(y)) => x.partial_cmp(y).unwrap_or(std::cmp::Ordering::Equal),
            (DataValue::Text(x), DataValue::Text(y)) => x.cmp(y),
            _ => std::cmp::Ordering::Equal,
        }
    }

    /// Sum values for window functions
    fn sum_values(&self, values: &[DataValue]) -> DataValue {
        let mut sum = 0.0;
        for value in values {
            match value {
                DataValue::Integer(i) => sum += *i as f64,
                DataValue::Real(r) => sum += *r,
                _ => {}
            }
        }
        DataValue::Real(sum)
    }

    /// Average values for window functions
    fn avg_values(&self, values: &[DataValue]) -> DataValue {
        if values.is_empty() {
            return DataValue::Null;
        }

        let mut sum = 0.0;
        let mut count = 0;
        for value in values {
            match value {
                DataValue::Integer(i) => {
                    sum += *i as f64;
                    count += 1;
                }
                DataValue::Real(r) => {
                    sum += *r;
                    count += 1;
                }
                _ => {}
            }
        }

        if count > 0 {
            DataValue::Real(sum / count as f64)
        } else {
            DataValue::Null
        }
    }

    /// Min values for window functions
    fn min_values(&self, values: &[DataValue]) -> DataValue {
        let mut min_val: Option<DataValue> = None;
        for value in values {
            min_val = Some(match min_val {
                None => value.clone(),
                Some(current_min) => self.min_of_values(&current_min, value),
            });
        }
        min_val.unwrap_or(DataValue::Null)
    }

    /// Max values for window functions
    fn max_values(&self, values: &[DataValue]) -> DataValue {
        let mut max_val: Option<DataValue> = None;
        for value in values {
            max_val = Some(match max_val {
                None => value.clone(),
                Some(current_max) => self.max_of_values(&current_max, value),
            });
        }
        max_val.unwrap_or(DataValue::Null)
    }

    /// Check if select list contains aggregate functions
    fn has_aggregate_functions(&self, select_list: &[SelectItem]) -> bool {
        for item in select_list {
            match item {
                SelectItem::Expression { expr, .. } => {
                    if self.expression_contains_aggregate(expr) {
                        return true;
                    }
                }
                _ => {}
            }
        }
        false
    }

    /// Check if expression contains aggregate functions
    fn expression_contains_aggregate(&self, expr: &Expression) -> bool {
        match expr {
            Expression::Function(FunctionCall { name, .. }) => {
                matches!(name.to_uppercase().as_str(), "COUNT" | "SUM" | "AVG" | "MIN" | "MAX")
            }
            Expression::BinaryOp(BinaryOp { left, right, .. }) => {
                self.expression_contains_aggregate(left) || self.expression_contains_aggregate(right)
            }
            _ => false,
        }
    }

    /// Check if select list contains window functions
    fn has_window_functions(&self, select_list: &[SelectItem]) -> bool {
        for item in select_list {
            match item {
                SelectItem::Expression { expr, .. } => {
                    if self.expression_contains_window_function(expr) {
                        return true;
                    }
                }
                _ => {}
            }
        }
        false
    }

    /// Check if expression contains window functions
    fn expression_contains_window_function(&self, expr: &Expression) -> bool {
        match expr {
            Expression::WindowFunction(_) => true,
            Expression::BinaryOp(BinaryOp { left, right, .. }) => {
                self.expression_contains_window_function(left) || self.expression_contains_window_function(right)
            }
            _ => false,
        }
    }

    /// Execute aggregation query with GROUP BY and aggregate functions
    async fn execute_aggregation_query(&self, select_query: &SelectQuery, rows: Vec<HashMap<String, DataValue>>) -> AuroraResult<QueryResult> {
        // Group rows by GROUP BY expressions
        let grouped_rows = if let Some(group_by) = &select_query.group_by {
            self.group_rows_by_expressions(&rows, &group_by.expressions)?
        } else {
            // No GROUP BY, treat all rows as one group
            vec![("single_group".to_string(), rows)]
        };

        // Apply aggregate functions to each group
        let mut result_rows = Vec::new();

        for (group_key, group_rows) in grouped_rows {
            let mut result_row = HashMap::new();

            // Process each select item
            for select_item in &select_query.select_list {
                match select_item {
                    SelectItem::Expression { expr, alias } => {
                        let value = self.evaluate_aggregate_expression(expr, &group_rows)?;
                        let column_name = alias.clone().unwrap_or_else(|| self.expression_to_column_name(expr));
                        result_row.insert(column_name, value);
                    }
                    SelectItem::Wildcard => {
                        // For aggregation queries, wildcard might not make sense
                        // Could expand to all group by columns + aggregates
                        continue;
                    }
                }
            }

            result_rows.push(result_row);
        }

        // Apply HAVING clause if present
        let final_rows = if let Some(having_expr) = &select_query.having {
            result_rows.into_iter()
                .filter(|row| {
                    match self.evaluate_having_expression(having_expr, row) {
                        Ok(val) => matches!(val, DataValue::Boolean(true)),
                        Err(_) => false,
                    }
                })
                .collect()
        } else {
            result_rows
        };

        Ok(QueryResult {
            rows: Some(final_rows),
            rows_affected: None,
            execution_time_ms: 0,
            query_plan: None,
        })
    }

    /// Group rows by GROUP BY expressions
    fn group_rows_by_expressions(&self, rows: &[HashMap<String, DataValue>], group_exprs: &[Expression]) -> AuroraResult<Vec<(String, Vec<HashMap<String, DataValue>>)>> {
        let mut groups: HashMap<String, Vec<HashMap<String, DataValue>>> = HashMap::new();

        for row in rows {
            let mut group_key_parts = Vec::new();

            for expr in group_exprs {
                let group_value = self.evaluate_group_expression(expr, row)?;
                group_key_parts.push(format!("{:?}", group_value));
            }

            let group_key = group_key_parts.join("|");
            groups.entry(group_key).or_insert_with(Vec::new).push(row.clone());
        }

        Ok(groups.into_iter().collect())
    }

    /// Evaluate aggregate expression on a group of rows
    fn evaluate_aggregate_expression(&self, expr: &Expression, group_rows: &[HashMap<String, DataValue>]) -> AuroraResult<DataValue> {
        match expr {
            Expression::Function(FunctionCall { name, arguments }) => {
                match name.to_uppercase().as_str() {
                    "COUNT" => {
                        if arguments.is_empty() || matches!(arguments[0], Expression::Asterisk) {
                            Ok(DataValue::Integer(group_rows.len() as i64))
                        } else {
                            // COUNT(column) - count non-null values
                            let mut count = 0;
                            for row in group_rows {
                                let arg_value = self.evaluate_group_expression(&arguments[0], row)?;
                                if !matches!(arg_value, DataValue::Null) {
                                    count += 1;
                                }
                            }
                            Ok(DataValue::Integer(count))
                        }
                    }
                    "SUM" => {
                        if arguments.is_empty() {
                            return Err(AuroraError::new(ErrorCode::QuerySyntax, "SUM requires an argument".to_string()));
                        }
                        let mut sum = 0.0;
                        let mut count = 0;
                        for row in group_rows {
                            let arg_value = self.evaluate_group_expression(&arguments[0], row)?;
                            match arg_value {
                                DataValue::Integer(i) => {
                                    sum += i as f64;
                                    count += 1;
                                }
                                DataValue::Real(r) => {
                                    sum += r;
                                    count += 1;
                                }
                                _ => {} // Skip non-numeric values
                            }
                        }
                        if count > 0 {
                            Ok(DataValue::Real(sum))
                        } else {
                            Ok(DataValue::Null)
                        }
                    }
                    "AVG" => {
                        if arguments.is_empty() {
                            return Err(AuroraError::new(ErrorCode::QuerySyntax, "AVG requires an argument".to_string()));
                        }
                        let mut sum = 0.0;
                        let mut count = 0;
                        for row in group_rows {
                            let arg_value = self.evaluate_group_expression(&arguments[0], row)?;
                            match arg_value {
                                DataValue::Integer(i) => {
                                    sum += i as f64;
                                    count += 1;
                                }
                                DataValue::Real(r) => {
                                    sum += r;
                                    count += 1;
                                }
                                _ => {} // Skip non-numeric values
                            }
                        }
                        if count > 0 {
                            Ok(DataValue::Real(sum / count as f64))
                        } else {
                            Ok(DataValue::Null)
                        }
                    }
                    "MIN" => {
                        if arguments.is_empty() {
                            return Err(AuroraError::new(ErrorCode::QuerySyntax, "MIN requires an argument".to_string()));
                        }
                        let mut min_value: Option<DataValue> = None;
                        for row in group_rows {
                            let arg_value = self.evaluate_group_expression(&arguments[0], row)?;
                            if !matches!(arg_value, DataValue::Null) {
                                min_value = Some(match min_value {
                                    None => arg_value,
                                    Some(current_min) => self.min_of_values(&current_min, &arg_value),
                                });
                            }
                        }
                        min_value.unwrap_or(DataValue::Null)
                    }
                    "MAX" => {
                        if arguments.is_empty() {
                            return Err(AuroraError::new(ErrorCode::QuerySyntax, "MAX requires an argument".to_string()));
                        }
                        let mut max_value: Option<DataValue> = None;
                        for row in group_rows {
                            let arg_value = self.evaluate_group_expression(&arguments[0], row)?;
                            if !matches!(arg_value, DataValue::Null) {
                                max_value = Some(match max_value {
                                    None => arg_value,
                                    Some(current_max) => self.max_of_values(&current_max, &arg_value),
                                });
                            }
                        }
                        max_value.unwrap_or(DataValue::Null)
                    }
                    _ => Err(AuroraError::new(ErrorCode::QuerySyntax, format!("Unknown aggregate function: {}", name))),
                }
            }
            // For non-aggregate expressions in GROUP BY context, evaluate normally
            _ => self.evaluate_group_expression(expr, &group_rows[0]), // Use first row for non-aggregates
        }
    }

    /// Evaluate expression on a single row for GROUP BY context
    fn evaluate_group_expression(&self, expr: &Expression, row: &HashMap<String, DataValue>) -> AuroraResult<DataValue> {
        match expr {
            Expression::Identifier(column) => {
                row.get(column).cloned().unwrap_or(DataValue::Null)
            }
            Expression::Literal(lit) => Ok(self.literal_to_datavalue(lit)),
            _ => Err(AuroraError::new(ErrorCode::QuerySyntax, "Complex expressions in GROUP BY not yet supported".to_string())),
        }
    }

    /// Evaluate HAVING clause expression
    fn evaluate_having_expression(&self, expr: &Expression, row: &HashMap<String, DataValue>) -> AuroraResult<DataValue> {
        // HAVING expressions can reference aggregate results
        // For now, simplified implementation
        match expr {
            Expression::BinaryOp(BinaryOp { left, op, right }) => {
                let left_val = self.evaluate_having_expression(left, row)?;
                let right_val = self.evaluate_having_expression(right, row)?;

                match op {
                    BinaryOperator::Equal => Ok(DataValue::Boolean(left_val == right_val)),
                    BinaryOperator::GreaterThan => self.compare_values(&left_val, &right_val, |a, b| a > b),
                    BinaryOperator::LessThan => self.compare_values(&left_val, &right_val, |a, b| a < b),
                    _ => Ok(DataValue::Boolean(false)), // Simplified
                }
            }
            Expression::Identifier(column) => {
                row.get(column).cloned().unwrap_or(DataValue::Null)
            }
            Expression::Literal(lit) => Ok(self.literal_to_datavalue(lit)),
            _ => Ok(DataValue::Boolean(false)), // Simplified
        }
    }

    /// Get minimum of two values
    fn min_of_values(&self, a: &DataValue, b: &DataValue) -> DataValue {
        match (a, b) {
            (DataValue::Integer(x), DataValue::Integer(y)) => DataValue::Integer(*x.min(y)),
            (DataValue::Real(x), DataValue::Real(y)) => DataValue::Real(x.min(*y)),
            (DataValue::Text(x), DataValue::Text(y)) => DataValue::Text(x.min(y).clone()),
            _ => a.clone(), // Type mismatch, return first value
        }
    }

    /// Get maximum of two values
    fn max_of_values(&self, a: &DataValue, b: &DataValue) -> DataValue {
        match (a, b) {
            (DataValue::Integer(x), DataValue::Integer(y)) => DataValue::Integer(*x.max(y)),
            (DataValue::Real(x), DataValue::Real(y)) => DataValue::Real(x.max(*y)),
            (DataValue::Text(x), DataValue::Text(y)) => DataValue::Text(x.max(y).clone()),
            _ => a.clone(), // Type mismatch, return first value
        }
    }

    /// Compare two values
    fn compare_values<F>(&self, a: &DataValue, b: &DataValue, cmp: F) -> AuroraResult<DataValue>
    where
        F: Fn(&DataValue, &DataValue) -> bool,
    {
        let result = match (a, b) {
            (DataValue::Integer(x), DataValue::Integer(y)) => cmp(&DataValue::Integer(*x), &DataValue::Integer(*y)),
            (DataValue::Real(x), DataValue::Real(y)) => cmp(&DataValue::Real(*x), &DataValue::Real(*y)),
            _ => false, // Type mismatch
        };
        Ok(DataValue::Boolean(result))
    }

    /// Convert expression to column name for display
    fn expression_to_column_name(&self, expr: &Expression) -> String {
        match expr {
            Expression::Identifier(name) => name.clone(),
            Expression::Function(FunctionCall { name, .. }) => name.clone(),
            Expression::Literal(_) => "literal".to_string(),
            Expression::BinaryOp(_) => "expression".to_string(),
            Expression::Column(name) => name.clone(),
            Expression::VectorLiteral(_) => "vector".to_string(),
            Expression::Asterisk => "*".to_string(),
        }
    }

    /// Convert literal to DataValue
    fn literal_to_datavalue(&self, lit: &crate::query::parser::ast::Literal) -> DataValue {
        match lit {
            crate::query::parser::ast::Literal::Integer(i) => DataValue::Integer(*i),
            crate::query::parser::ast::Literal::Float(f) => DataValue::Real(*f),
            crate::query::parser::ast::Literal::String(s) => DataValue::Text(s.clone()),
            crate::query::parser::ast::Literal::Boolean(b) => DataValue::Boolean(*b),
            crate::query::parser::ast::Literal::Null => DataValue::Null,
        }
    }

    /// Execute regular (non-aggregation) SELECT query
    async fn execute_regular_select(&self, select_query: &SelectQuery, rows: Vec<HashMap<String, DataValue>>) -> AuroraResult<QueryResult> {
        // Apply column selection
        let mut result_rows = Vec::new();
        for row in rows {
            let mut result_row = HashMap::new();

            // Handle SELECT * or specific columns
            let columns_to_select = if select_query.select_list.iter().any(|item| matches!(item, SelectItem::Wildcard)) {
                // SELECT * - get all columns
                row.keys().cloned().collect()
            } else {
                // SELECT specific columns
                select_query.select_list.iter()
                    .filter_map(|item| match item {
                        SelectItem::Expression { expr, alias } => {
                            // Simplified: just get column name from expression
                            match expr {
                                Expression::Identifier(column_name) => Some(column_name.clone()),
                                _ => None, // Complex expressions not yet supported
                            }
                        }
                        _ => None,
                    })
                    .collect()
            };

            // Extract selected columns
            for column_name in columns_to_select {
                if let Some(value) = row.get(&column_name) {
                    result_row.insert(column_name, value.clone());
                }
            }

            result_rows.push(result_row);
        }

        // Apply LIMIT if specified
        let final_rows = if let Some(limit) = select_query.limit {
            result_rows.into_iter().take(limit as usize).collect()
        } else {
            result_rows
        };

        Ok(QueryResult {
            rows: Some(final_rows),
            rows_affected: None,
            execution_time_ms: 0,
            query_plan: None,
        })
    }

    /// Apply WHERE clause filtering for MVCC data
    fn apply_where_clause_mvcc(&self, rows: &[HashMap<String, DataValue>], where_clause: &Expression) -> AuroraResult<Vec<HashMap<String, DataValue>>> {
        let mut filtered = Vec::new();

        for row in rows {
            // Simplified WHERE clause evaluation
            // TODO: Implement full expression evaluation
            if self.evaluate_where_condition_mvcc(row, where_clause)? {
                filtered.push(row.clone());
            }
        }

        Ok(filtered)
    }

    /// Evaluate WHERE condition for MVCC row data (simplified)
    fn evaluate_where_condition_mvcc(&self, row: &HashMap<String, DataValue>, condition: &Expression) -> AuroraResult<bool> {
        match condition {
            Expression::BinaryOp { left, op, right } => {
                // Simple binary operations like "id = 1"
                match (left.as_ref(), op, right.as_ref()) {
                    (Expression::Identifier(column_name), BinaryOperator::Equal, Expression::Literal(literal)) => {
                        if let Some(row_value) = row.get(column_name) {
                            match (row_value, literal) {
                                (DataValue::Integer(row_int), Literal::Integer(lit_int)) => Ok(row_int == lit_int),
                                (DataValue::Text(row_text), Literal::String(lit_text)) => Ok(row_text == lit_text),
                                (DataValue::Boolean(row_bool), Literal::Boolean(lit_bool)) => Ok(row_bool == lit_bool),
                                _ => Ok(false), // Type mismatch
                            }
                        } else {
                            Ok(false) // Column not found
                        }
                    }
                    _ => {
                        log::warn!("Complex WHERE conditions not yet implemented");
                        Ok(true) // Accept all for now
                    }
                }
            }
            _ => {
                log::warn!("Complex WHERE conditions not yet implemented");
                Ok(true) // Accept all for now
            }
        }
    }

    /// Execute an analytics query with advanced processing
    pub async fn execute_analytics(&self, query: &AnalyticsQuery, user_context: &UserContext) -> AuroraResult<AnalyticsResult> {
        let start_time = std::time::Instant::now();

        // Access control for analytics
        self.access_controller.check_analytics_access(query, user_context).await?;

        // Audit logging
        self.audit_logger.log_analytics_query(query, user_context).await?;

        // Execute analytics query
        let result = self.execution_engine.execute_analytics(query).await?;

        // Update metrics
        let execution_time = start_time.elapsed();
        self.metrics_collector.record_analytics_query(execution_time).await?;

        Ok(result)
    }

    /// Begin a new transaction
    pub async fn begin_transaction(&self, isolation_level: IsolationLevel, user_context: &UserContext) -> AuroraResult<Arc<Transaction>> {
        // Access control
        self.access_controller.check_transaction_access(user_context).await?;

        // Create transaction
        let transaction = self.transaction_manager.begin_transaction(isolation_level).await?;

        // Store in active transactions
        let tx_id = transaction.get_id().to_string();
        self.active_transactions.write().insert(tx_id.clone(), transaction.clone());

        // Audit logging
        self.audit_logger.log_transaction_begin(&tx_id, user_context).await?;

        Ok(transaction)
    }

    /// Commit a transaction
    pub async fn commit_transaction(&self, transaction: Arc<Transaction>, user_context: &UserContext) -> AuroraResult<()> {
        let tx_id = transaction.get_id().to_string();

        // Execute commit
        self.transaction_manager.commit_transaction(transaction).await?;

        // Remove from active transactions
        self.active_transactions.write().remove(&tx_id);

        // Audit logging
        self.audit_logger.log_transaction_commit(&tx_id, user_context).await?;

        Ok(())
    }

    /// Rollback a transaction
    pub async fn rollback_transaction(&self, transaction: Arc<Transaction>, user_context: &UserContext) -> AuroraResult<()> {
        let tx_id = transaction.get_id().to_string();

        // Execute rollback
        self.transaction_manager.rollback_transaction(transaction).await?;

        // Remove from active transactions
        self.active_transactions.write().remove(&tx_id);

        // Audit logging
        self.audit_logger.log_transaction_rollback(&tx_id, user_context).await?;

        Ok(())
    }

    /// Create a table with the specified schema
    pub async fn create_table(&self, table_name: &str, schema: &TableSchema, user_context: &UserContext) -> AuroraResult<()> {
        // Access control
        self.access_controller.check_ddl_access(table_name, user_context).await?;

        // Validate schema
        self.validate_table_schema(schema).await?;

        // Create table in storage
        self.storage_manager.create_table(table_name, schema).await?;

        // Create vector index if needed
        if schema.has_vector_columns() {
            self.vector_index_manager.create_table_index(table_name, schema).await?;
        }

        // Audit logging
        self.audit_logger.log_ddl_operation("CREATE TABLE", table_name, user_context).await?;

        Ok(())
    }

    /// Drop a table
    pub async fn drop_table(&self, table_name: &str, user_context: &UserContext) -> AuroraResult<()> {
        // Access control
        self.access_controller.check_ddl_access(table_name, user_context).await?;

        // Drop vector index if exists
        self.vector_index_manager.drop_table_index(table_name).await?;

        // Drop table from storage
        self.storage_manager.drop_table(table_name).await?;

        // Audit logging
        self.audit_logger.log_ddl_operation("DROP TABLE", table_name, user_context).await?;

        Ok(())
    }

    /// Get database health status
    pub async fn get_health_status(&self) -> AuroraResult<HealthStatus> {
        self.health_checker.check_health().await
    }

    /// Get database metrics
    pub async fn get_metrics(&self) -> AuroraResult<DatabaseMetrics> {
        let query_count = self.query_count.load(std::sync::atomic::Ordering::Relaxed);
        let total_query_time = self.total_query_time.load(std::sync::atomic::Ordering::Relaxed);

        let avg_query_time = if query_count > 0 {
            total_query_time / query_count
        } else {
            0
        };

        Ok(DatabaseMetrics {
            total_queries: query_count,
            average_query_time_micros: avg_query_time,
            active_transactions: self.active_transactions.read().len(),
            storage_metrics: self.storage_manager.get_metrics().await?,
            vector_metrics: self.vector_engine.get_metrics().await?,
            health_status: self.get_health_status().await?,
        })
    }

    /// Shutdown the database gracefully
    pub async fn shutdown(&self) -> AuroraResult<()> {
        println!("ðŸ›‘ Shutting down AuroraDB Production Database Engine...");

        // Wait for active transactions to complete
        let active_count = self.active_transactions.read().len();
        if active_count > 0 {
            println!("   Waiting for {} active transactions to complete...", active_count);
            // In practice, we'd wait with a timeout
        }

        // Flush all storage engines
        self.storage_manager.flush_all().await?;

        // Close vector indices
        self.vector_index_manager.close_all().await?;

        // Final metrics collection
        let final_metrics = self.get_metrics().await?;
        println!("   Final metrics: {} queries processed, {}Î¼s avg query time",
                final_metrics.total_queries, final_metrics.average_query_time_micros);

        println!("âœ… AuroraDB Production Database Engine shutdown complete!");
        Ok(())
    }

    // Private helper methods
    async fn perform_startup_checks(&self) -> AuroraResult<()> {
        // Check storage engines
        self.storage_manager.perform_integrity_checks().await?;

        // Check vector indices
        self.vector_index_manager.validate_indices().await?;

        // Check security policies
        self.access_controller.validate_policies().await?;

        Ok(())
    }

    fn should_cache_query(&self, sql: &str) -> bool {
        // Simple heuristic: cache SELECT queries without parameters
        sql.trim().to_uppercase().starts_with("SELECT") &&
        !sql.contains('?') &&
        sql.len() < 1000
    }

    async fn validate_table_schema(&self, schema: &TableSchema) -> AuroraResult<()> {
        // Basic schema validation
        if schema.columns.is_empty() {
            return Err(AuroraError::SchemaError("Table must have at least one column".to_string()));
        }

        // Check for duplicate column names
        let mut column_names = std::collections::HashSet::new();
        for column in &schema.columns {
            if !column_names.insert(column.name.clone()) {
                return Err(AuroraError::SchemaError(format!("Duplicate column name: {}", column.name)));
            }
        }

        Ok(())
    }
}

/// User context for access control and auditing
#[derive(Debug, Clone)]
pub struct UserContext {
    pub user_id: String,
    pub username: String,
    pub roles: Vec<String>,
    pub client_ip: Option<String>,
    pub session_id: String,
}

/// Query result from database execution
#[derive(Debug, Clone)]
pub struct QueryResult {
    pub columns: Vec<String>,
    pub rows: Vec<Vec<serde_json::Value>>,
    pub execution_time: std::time::Duration,
    pub rows_affected: Option<u64>,
    pub query_plan: Option<String>,
}

/// Vector search request
#[derive(Debug, Clone)]
pub struct VectorSearchRequest {
    pub collection: String,
    pub query_vector: Vec<f32>,
    pub limit: usize,
    pub filters: Option<HashMap<String, serde_json::Value>>,
    pub include_metadata: bool,
}

/// Vector search result
#[derive(Debug, Clone)]
pub struct VectorSearchResult {
    pub results: Vec<VectorSearchHit>,
    pub execution_time: std::time::Duration,
    pub total_candidates: usize,
}

/// Vector search hit
#[derive(Debug, Clone)]
pub struct VectorSearchHit {
    pub id: String,
    pub score: f32,
    pub metadata: Option<HashMap<String, serde_json::Value>>,
}

/// Analytics query
#[derive(Debug, Clone)]
pub struct AnalyticsQuery {
    pub sql: String,
    pub window_spec: Option<WindowSpecification>,
    pub aggregation_functions: Vec<String>,
}

/// Analytics result
#[derive(Debug, Clone)]
pub struct AnalyticsResult {
    pub data: Vec<HashMap<String, serde_json::Value>>,
    pub execution_time: std::time::Duration,
    pub insights: Vec<String>,
}

/// Isolation level for transactions
#[derive(Debug, Clone)]
pub enum IsolationLevel {
    ReadUncommitted,
    ReadCommitted,
    RepeatableRead,
    Serializable,
}

/// Table schema definition
#[derive(Debug, Clone)]
pub struct TableSchema {
    pub columns: Vec<ColumnDefinition>,
    pub primary_key: Option<Vec<String>>,
    pub indexes: Vec<IndexDefinition>,
}

impl TableSchema {
    fn has_vector_columns(&self) -> bool {
        self.columns.iter().any(|col| matches!(col.data_type, DataType::Vector(_)))
    }
}

/// Column definition
#[derive(Debug, Clone)]
pub struct ColumnDefinition {
    pub name: String,
    pub data_type: DataType,
    pub nullable: bool,
    pub default_value: Option<serde_json::Value>,
}

/// Data types supported by AuroraDB
#[derive(Debug, Clone)]
pub enum DataType {
    Integer,
    BigInt,
    Float,
    Double,
    Text,
    Boolean,
    Timestamp,
    Json,
    Vector(usize), // Dimension size
    Array(Box<DataType>),
}

/// Index definition
#[derive(Debug, Clone)]
pub struct IndexDefinition {
    pub name: String,
    pub columns: Vec<String>,
    pub index_type: IndexType,
}

/// Index types
#[derive(Debug, Clone)]
pub enum IndexType {
    BTree,
    Hash,
    Vector,
    FullText,
    Spatial,
}

/// Window specification for analytics
#[derive(Debug, Clone)]
pub struct WindowSpecification {
    pub window_type: WindowType,
    pub size: Option<String>,
    pub slide: Option<String>,
}

/// Window types
#[derive(Debug, Clone)]
pub enum WindowType {
    Tumbling,
    Sliding,
    Session,
}

/// Health status of the database
#[derive(Debug, Clone)]
pub struct HealthStatus {
    pub overall_status: HealthState,
    pub component_statuses: HashMap<String, HealthState>,
    pub last_check: std::time::SystemTime,
}

/// Health states
#[derive(Debug, Clone)]
pub enum HealthState {
    Healthy,
    Degraded,
    Unhealthy,
}

/// Database metrics
#[derive(Debug, Clone)]
pub struct DatabaseMetrics {
    pub total_queries: u64,
    pub average_query_time_micros: u64,
    pub active_transactions: usize,
    pub storage_metrics: crate::storage::StorageMetrics,
    pub vector_metrics: crate::vector::VectorMetrics,
    pub health_status: HealthStatus,
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_aurora_db_initialization() {
        // This would require a full test setup with all components
        // For now, just test that the struct can be created conceptually
        assert!(true); // Placeholder - full integration tests would be complex
    }
}
